<h1 align="center">自然语言处理与词嵌入</h1>

## 词嵌入

one-hot 向量将每个单词表示为完全独立的个体，不同词向量都是正交的，因此单词间的相似度无法体现。

换用特征化表示方法能够解决这一问题。我们可以通过用语义特征作为维度来表示一个词，因此语义相近的词，其词向量也相近。

将高维的词嵌入“嵌入”到一个二维空间里，就可以进行可视化。常用的一种可视化算法是 t-SNE 算法。在通过复杂而非线性的方法映射到二维空间后，每个词会根据语义和相关程度聚在一起。相关论文：[van der Maaten and Hinton., 2008. Visualizing Data using t-SNE](https://www.seas.harvard.edu/courses/cs281/papers/tsne.pdf)

**词嵌入（Word Embedding）**是 NLP 中语言模型与表征学习技术的统称，概念上而言，它是指把一个维数为所有词的数量的高维空间（one-hot 形式表示的词）“嵌入”到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。对大量词汇进行词嵌入后获得的词向量，可用于完成**命名实体识别（Named Entity Recognition）**等任务。

### 词嵌入与迁移学习

用词嵌入做迁移学习可以降低学习成本，提高效率。其步骤如下：

1. 从大量的文本集中学习词嵌入，或者下载网上开源的、预训练好的词嵌入模型；
2. 将这些词嵌入模型迁移到新的、只有少量标注训练集的任务中；
3. 可以选择是否微调词嵌入。当标记数据集不是很大时可以省下这一步。

### 词嵌入与类比推理

词嵌入可用于类比推理。例如，给定对应关系“男性（Man）”对“女性（Woman）”，想要类比出“国王（King）”对应的词汇。则可以有 $e\_{man} - e\_{woman} \approx e\_{king} - e\_? $ ，之后的目标就是找到词向量 $w$，来找到使相似度 $sim(e\_w, e\_{king} - e\_{man} + e\_{woman})$ 最大。

一个最常用的相似度计算函数是**余弦相似度（cosine similarity）**。公式为：

$$sim(u, v) = \frac{u^T v}{|| u ||\_2 || v ||\_2}$$

相关论文：[Mikolov et. al., 2013, Linguistic regularities in continuous space word representations](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)

## 嵌入矩阵

不同的词嵌入方法能够用不同的方式学习到一个**嵌入矩阵（Embedding Matrix）** $E$。将字典中位置为 $i$ 的词的 one-hot 向量表示为 $o\_i$，词嵌入后生成的词向量用 $e\_i$表示，则有：

$$E \cdot o\_i = e\_i$$

但在实际情况下一般不这么做。因为 one-hot 向量维度很高，且几乎所有元素都是 0，这样做的效率太低。因此，实践中直接用专门的函数查找矩阵 $E$ 的特定列。

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']]}
});
</script>

<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>