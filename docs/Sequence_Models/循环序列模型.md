<h1 align="center">循环序列模型</h1>

自然语言和音频都是前后相互关联的数据，对于这些序列数据需要使用**循环神经网络（Recurrent Neural Network，RNN）**来进行处理。

使用 RNN 实现的应用包括下图中所示：

![Examples-of-Sequence-Model](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Sequence_Models/Examples-of-Sequence-Model.png)

## 数学符号

对于一个序列数据 $x$，用符号 $x^{⟨t⟩}$来表示这个数据中的第 $t$个元素，用 $y^{⟨t⟩}$来表示第 $t$个标签，用 $T\_x$ 和 $T\_y$来表示输入和输出的长度。对于一段音频，元素可能是其中的几帧；对于一句话，元素可能是一到多个单词。

第 $i$ 个序列数据的第 $t$ 个元素用符号 $x^{(i)⟨t⟩}$，第 $t$ 个标签即为 $y^{(i)⟨t⟩}$。对应即有 $T^{(i)}\_x$ 和 $T^{(i)}\_y$。

想要表示一个词语，需要先建立一个**词汇表（Vocabulary）**，或者叫**字典（Dictionary）**。将需要表示的所有词语变为一个列向量，可以根据字母顺序排列，然后根据单词在向量中的位置，用 **one-hot 向量（one-hot vector）**来表示该单词的标签：将每个单词编码成一个 $R^{|V| \times 1}$向量，其中 $|V|$是词汇表中单词的数量。一个单词在词汇表中的索引在该向量对应的元素为 1，其余元素均为 0。

例如，'zebra'排在词汇表的最后一位，因此它的词向量表示为：

$$w^{zebra} = \left [ 0, 0, 0, ..., 1\right ]^T$$

**补充：**one-hot 向量是最简单的词向量。它的**缺点**是，由于每个单词被表示为完全独立的个体，因此单词间的相似度无法体现。例如单词 hotel 和 motel 意思相近，而与 cat 不相似，但是

$$(w^{hotel})^Tw^{motel} = (w^{hotel})^Tw^{cat} = 0$$

## 循环神经网络模型

对于序列数据，使用标准神经网络存在以下问题：

* 对于不同的示例，输入和输出可能有不同的长度，因此输入层和输出层的神经元数量无法固定。
* 从输入文本的不同位置学到的同一特征无法共享。
* 模型中的参数太多，计算量太大。

为了解决这些问题，引入**循环神经网络（Recurrent Neural Network，RNN）**。一种循环神经网络的结构如下图所示：

![Recurrent-Neural-Network](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Sequence_Models/Recurrent-Neural-Network.png)

当元素 $x^{⟨t⟩}$ 输入对应时间步（Time Step）的隐藏层的同时，该隐藏层也会接收来自上一时间步的隐藏层的激活值 $a^{⟨t-1⟩}$，其中 $a^{⟨0⟩}$ 一般直接初始化为零向量。一个时间步输出一个对应的预测结果 $\hat y^{⟨t⟩}$。

循环神经网络从左向右扫描数据，同时每个时间步的参数也是共享的，输入、激活、输出的参数对应为 $W\_{ax}$、$W\_{aa}$、$W\_{ay}$。

目前我们看到的模型的问题是，只使用了这个序列中之前的信息来做出预测，即后文没有被使用。可以通过**双向循环神经网络（Bidirectional RNN，BRNN）**来解决这个问题。

前向传播过程的公式如下：

$$a^{⟨0⟩} = \vec{0}$$

$$a^{⟨t⟩} = g\_1(W\_{aa}a^{⟨t-1⟩} + W\_{ax}x^{⟨t⟩} + b\_a)$$

$$\hat y^{⟨t⟩} = g\_2(W\_{ya}a^{⟨t⟩} + b\_y)$$

激活函数 $g\_1$通常选择 tanh，有时也用 ReLU；$g\_2$可选 sigmoid 或 softmax，取决于需要的输出类型。

为了进一步简化公式以方便运算，可以将 $W\_{ax}$、$W\_{aa}$**水平并列**为一个矩阵 $W\_a$，同时 $a^{⟨t-1⟩}$和 $ x^{⟨t⟩}$ **堆叠**成一个矩阵。则有：

$$W\_a = [W\_{ax}, W\_{aa}]$$

$$a^{⟨t⟩} = g\_1(W\_a[a^{⟨t-1⟩}, x^{⟨t⟩}] + b\_a)$$

$$\hat y^{⟨t⟩} = g\_2(W\_{y}a^{⟨t⟩} + b\_y)$$

### 反向传播

为了计算反向传播过程，需要先定义一个损失函数。单个位置上（或者说单个时间步上）某个单词的预测值的损失函数采用**交叉熵损失函数**，如下所示：

$$L^{⟨t⟩}(\hat y^{⟨t⟩}, y^{⟨t⟩}) = -y^{⟨t⟩}log\hat y^{⟨t⟩} - (1 - y^{⟨t⟩})log(1-\hat y^{⟨t⟩})$$

将单个位置上的损失函数相加，得到整个序列的成本函数如下：

$$J = L(\hat y, y) = \sum^{T\_x}\_{t=1} L^{⟨t⟩}(\hat y^{⟨t⟩}, y^{⟨t⟩})$$

循环神经网络的反向传播被称为**通过时间反向传播（Backpropagation through time）**，因为从右向左计算的过程就像是时间倒流。

更详细的计算公式如下：

![formula-of-RNN](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Sequence_Models/formula-of-RNN.png)

### 不同结构

某些情况下，输入长度和输出长度不一致。根据所需的输入及输出长度，循环神经网络可分为“一对一”、“多对一”、“多对多”等结构：

![Examples-of-RNN-architectures](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Sequence_Models/Examples-of-RNN-architectures.png)

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']]}
});
</script>

<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>







