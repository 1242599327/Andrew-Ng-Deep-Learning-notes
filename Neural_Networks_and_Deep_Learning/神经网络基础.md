<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 神经网络基础

* 实现一个神经网络时，如果需要遍历整个训练集，并不需要直接使用 for 循环。
* 神经网络的计算过程中，通常有一个正向过程（forward pass）或者叫**正向传播步骤（forward propagation step）**，接着会有一个反向过程（backward pass）或者叫**反向传播步骤（backward  propagation step）**。

## logistic 回归

Logistic 回归是一个用于二分分类的算法。

Logistic 回归中使用的参数如下：

* 输入的特征向量：$$x \in R^{n_x}$$

其中

$${n_x}$$ 是特征数量；

* 用于训练的标签：$$y \in 0,1$$
* 权重：$$w \in R^{n_x}$$
* threshold： $$b \in R$$
* 输出：$$\hat{y} = \sigma(w^Tx+b)$$
* Sigmoid 函数：$$s = \sigma(w^Tx+b) = \sigma(z) = \frac{1}{1+e^{-z}}$$

为将$$w^Tx+b$$

约束在 [0, 1] 间，引入 Sigmoid 函数。从下图可看出，Sigmoid 函数的值域为 [0, 1]。

![sigmoid-function](sigmoid-function.png)

Logistic 回归可以看作是一个非常小的神经网络。

### 损失函数

**损失函数（loss function）**用于衡量预测结果与真实值之间的误差。

最简单的损失函数定义方式为平方差损失：$$L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^2$$

但 Logistic 回归中我们并不倾向于使用这样的损失函数，因为之后讨论的优化问题会变成非凸的，最后会得到很多个局部最优解，梯度下降法可能找不到全局最优值。

一般使用$$L(\hat{y},y) = -(y\log\hat{y})+(1-y)\log(1-\hat{y})$$

损失函数是在单个训练样本中定义的，它衡量了在**单个**训练样本上的表现。而**成本函数（cost function，或者称作代价函数）**衡量的是在**全体**训练样本上的表现。

$$J(w,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y},y)$$

### 参考资料

[二元分类与 Logistic 回归 - 某熊的全栈之路](https://segmentfault.com/a/1190000010671531)

