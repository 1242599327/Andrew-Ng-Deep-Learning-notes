<h1 align="center">优化算法</h1>

深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。优化算法能够帮助快速训练模型，大大提高效率。

## Mini-batch 梯度下降法

### batch 梯度下降法

**batch 梯度下降法**（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新。

对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢。

但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 **mini-batch**。

### Mini-Batch 梯度下降法

**Mini-Batch 梯度下降法**（小批量梯度下降法）每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。

使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（一代，an epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。

batch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：

![training-with-mini-batch-gradient-descent](training-with-mini-batch-gradient-descent.png)

### batch 的不同大小（size）带来的影响

* mini-batch 的大小为 1，即是**随机梯度下降法（stochastic gradient descent）**，每个样本都是独立的 mini-batch；
* mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法；

![choosing-mini-batch-size](choosing-mini-batch-size.png)

* batch 梯度下降法：
    * 对所有 m 个训练样本执行一次梯度下降，**每一次迭代时间较长**； 
    * 相对噪声低一些，幅度也大一些；
    * 成本函数总是向减小的方向下降。

* 随机梯度下降法：
    * 对每一个训练样本执行一次梯度下降，**丢失了向量化带来的计算加速**；
    * 有很多噪声，减小学习率可以适当；
    * 成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。

因此，选择一个`1 < size < m`的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。

### mini-batch 大小的选择

* 如果训练样本的大小比较小，如 m ⩽ 2000 时，选择 batch 梯度下降法；
* 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。考虑到电脑内存设置和使用的方式，代码在 mini-batch 大小为 2 的平方时运行要快一些。典型的大小为 2^6、2^7、...、2^9；
* mini-batch 的大小要符合 CPU/GPU 内存。

mini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。

### 符号表示

* 使用上角小括号 i 表示训练集里的值，x^(i) 是第 i 个训练样本；
* 使用上角中括号 l 表示神经网络的层数，z^[l] 表示神经网络中第 l 层的 z 值；
* 现在引入大括号 t 来代表不同的 mini-batch，因此有 X^{t}、Y^{t}。

### 参考资料

* [吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（2-2）-- 优化算法](http://blog.csdn.net/koala_tree/article/details/78199611) 
* [深度学习(4)：优化神经网络(2) | Weber](http://binweber.top/2017/10/06/deep_learning_4/)

<span style="float: left;">上一节 <a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/Improving_Deep_Neural_Networks/Practical_aspects_of_Deep_Learning">深度学习的实用层面</a></span>

<span style="float: right;">下一节 <a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/Improving_Deep_Neural_Networks/Hyperparameter_tuning_Batch_Normalization_and_Programming_Frameworks">超参数调试、Batch 正则化和程序框架</a></span>

<p align="center" style="margin-top: 50px">
@Kyon Huang <a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/">首页</a> | <a href="https://github.com/bighuang624/Andrew-Ng-Deep-Learning-notes">Github</a>
</p>

<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>